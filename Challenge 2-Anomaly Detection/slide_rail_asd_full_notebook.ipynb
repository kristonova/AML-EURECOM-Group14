{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c6c84a",
   "metadata": {},
   "source": [
    "# Slide‑Rail Acoustic Anomaly Detection – Advanced Models\n",
    "This notebook extends the baseline implementation by experimenting with advanced models like CAE, VAE, Normalizing Flow, MAE, and Deep SVDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04429fd1",
   "metadata": {},
   "source": [
    "### Environment & Library Setup\n",
    "This section installs or imports the libraries required for audio processing, model training, and evaluation. It ensures reproducibility by pinning key package versions where feasible. Running these commands at the outset prevents dependency issues downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ed453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --quiet librosa==0.10.1 torch torchvision torchaudio scikit-learn tqdm kaggle \n",
    "import sys, os, json, zipfile, shutil, math, random, warnings, itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import mixture, metrics\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85740dc3",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "Here we load the labeled data from CSV files for supervised learning. The CSV files provide the class labels (0 for normal, 1 for anomaly) for both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2270dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded: (2370, 4)\n",
      "Testing data loaded: (1101, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load labeled data from CSV files\n",
    "train_csv_path = \"/Users/mymac/Study Abroad/Master Computer Science EURECOM/AML/Lab/AML-EURECOM-Group14/Challenge 2-Anomaly Detection/file_list_dev_data_train.csv\"\n",
    "test_csv_path = \"/Users/mymac/Study Abroad/Master Computer Science EURECOM/AML/Lab/AML-EURECOM-Group14/Challenge 2-Anomaly Detection/file_list_dev_data_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "print(\"Training data loaded:\", train_df.shape)\n",
    "print(\"Testing data loaded:\", test_df.shape)\n",
    "\n",
    "DATA_DIR = \"/Users/mymac/Study Abroad/Master Computer Science EURECOM/AML/Lab/AML-EURECOM-Group14/Challenge 2-Anomaly Detection/dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e8f27",
   "metadata": {},
   "source": [
    "### Feature Extraction: Log‑Mel Spectrograms\n",
    "We extract log‑Mel spectrograms for each audio file listed in the CSV files. These features serve as input to the supervised learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf512dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed.\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "N_FFT, HOP, N_MELS = 1024, 512, 64\n",
    "\n",
    "def extract_logmelspec(path, cache_dir=Path(DATA_DIR) / 'features'):\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cache_path = cache_dir / (path.stem + '.npy')\n",
    "    if cache_path.exists():\n",
    "        return np.load(cache_path)\n",
    "    y, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS, power=2\n",
    "    )\n",
    "    logmel = librosa.power_to_db(mel).astype(np.float32)\n",
    "    np.save(cache_path, logmel)\n",
    "    return logmel\n",
    "\n",
    "def extract_features_from_csv(df, folder_path, cache_dir=Path(DATA_DIR) / 'features'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = Path(folder_path) / row[\"original_filename\"]\n",
    "        logmel = extract_logmelspec(file_path, cache_dir=cache_dir)\n",
    "        features.append(logmel)\n",
    "        labels.append(row[\"class\"])\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Extract features and labels for training and testing\n",
    "train_features, train_labels = extract_features_from_csv(train_df, \n",
    "    \"/Users/mymac/Study Abroad/Master Computer Science EURECOM/AML/Lab/AML-EURECOM-Group14/Challenge 2-Anomaly Detection/dataset/dev_data/dev_data/slider/train\")\n",
    "test_features, test_labels = extract_features_from_csv(test_df, \n",
    "    \"/Users/mymac/Study Abroad/Master Computer Science EURECOM/AML/Lab/AML-EURECOM-Group14/Challenge 2-Anomaly Detection/dataset/dev_data/dev_data/slider/test\")\n",
    "\n",
    "print(\"Feature extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08ce99",
   "metadata": {},
   "source": [
    "### Torch Dataset Wrapper\n",
    "We create a custom `Dataset` class to handle the supervised learning data. This class provides both features and labels for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca6b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedSlideRailDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.features[idx]).unsqueeze(0)  # Add channel dimension\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = SupervisedSlideRailDataset(train_features, train_labels)\n",
    "test_dataset = SupervisedSlideRailDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe54d37",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoder (CAE) Baseline\n",
    "We train a CAE for 20-30 epochs and measure the internal AUC for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf35841",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (312) must match the size of tensor b (313) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m recon \u001b[38;5;241m=\u001b[39m cae_model(x)\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcrit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (312) must match the size of tensor b (313) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        # Crop output to match input dimensions if necessary\n",
    "        if out.size(2) != x.size(2) or out.size(3) != x.size(3):\n",
    "            out = out[:, :, :x.size(2), :x.size(3)]\n",
    "        return out\n",
    "\n",
    "cae_model = CAE().to(device)\n",
    "opt = torch.optim.Adam(cae_model.parameters(), lr=1e-3)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# Train CAE\n",
    "for epoch in range(20):\n",
    "    cae_model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "        recon = cae_model(x)\n",
    "        loss = crit(recon, x)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Measure AUC\n",
    "cae_model.eval()\n",
    "scores, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        recon = cae_model(x)\n",
    "        loss = ((recon - x) ** 2).mean(dim=(1, 2, 3))\n",
    "        scores.extend(loss.cpu().numpy())\n",
    "        labels.extend(y.numpy())\n",
    "auc = metrics.roc_auc_score(labels, scores)\n",
    "print(f\"CAE AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ffcfb",
   "metadata": {},
   "source": [
    "### Variational Autoencoder (VAE)\n",
    "We replace the reconstruction loss with a VAE loss to potentially improve AUC by 2-3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c2d4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x39936 and 8192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m recon, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mvae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(recon, x, mu, logvar)\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_logvar(z)\n\u001b[1;32m     19\u001b[0m     z_sample \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m logvar) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(logvar)\n\u001b[1;32m     20\u001b[0m     recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_fc(z_sample)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x39936 and 8192x128)"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc_logvar = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.decoder_fc = nn.Linear(128, 32 * 16 * 16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 2, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n",
    "        z_sample = mu + torch.exp(0.5 * logvar) * torch.randn_like(logvar)\n",
    "        recon = self.decoder(self.decoder_fc(z_sample).view(x.size(0), 32, 16, 16))\n",
    "        return recon, mu, logvar\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "opt = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# VAE Loss\n",
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = ((recon - x) ** 2).mean()\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div / x.size(0)\n",
    "\n",
    "# Train VAE\n",
    "for epoch in range(20):\n",
    "    vae_model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "        recon, mu, logvar = vae_model(x)\n",
    "        loss = vae_loss(recon, x, mu, logvar)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebd5a4",
   "metadata": {},
   "source": [
    "### Normalizing Flow or Masked Autoencoder (MAE)\n",
    "For advanced experiments, consider Normalizing Flow or MAE for potential performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd666996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores computed.\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for Normalizing Flow or MAE implementation\n",
    "# These models require significant computational resources and are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7510a",
   "metadata": {},
   "source": [
    "### Deep SVDD\n",
    "A one-class anomaly detection method using a lightweight model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSVDD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(x.size(0), -1)\n",
    "\n",
    "svdd_model = DeepSVDD().to(device)\n",
    "center = torch.zeros(32).to(device)\n",
    "\n",
    "# Train Deep SVDD\n",
    "opt = torch.optim.Adam(svdd_model.parameters(), lr=1e-3)\n",
    "for epoch in range(20):\n",
    "    svdd_model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "        z = svdd_model(x)\n",
    "        loss = ((z - center) ** 2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
